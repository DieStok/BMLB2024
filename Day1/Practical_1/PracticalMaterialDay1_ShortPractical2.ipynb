{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morning practical 2:\n",
    "\n",
    "In the first practical, you worked on getting the basics of hypothesis functions and gradient descent down. This was probably somewhat difficult, but extending this towards polynomial regression will be relatively easier now that you have the basis. Let's move towards polynomial regression. First, run the two code cells below, then move on. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary functions you defined before:\n",
    "def univariateHypothesis(x, thetas):\n",
    "    predict = thetas[0] + thetas[1] * x\n",
    "    return predict\n",
    "\n",
    "def MyMSE(x, y, thetas):\n",
    "    totalSumSquares = 0\n",
    "    for index, val in enumerate(x):\n",
    "        prediction = univariateHypothesis(val, thetas)\n",
    "        squareError = (prediction-y[index])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(x) \n",
    "    return meanSquaredError\n",
    "\n",
    "def gradientDescent(x, y, thetas, alpha):\n",
    "    m = len(x)\n",
    "    #print(\"m: \"); print(m)\n",
    "    totalErrorThetaZero = 0\n",
    "    totalErrorThetaOne = 0\n",
    "    for index, x_val in enumerate(x):\n",
    "        errorThetaZero = univariateHypothesis(x_val, thetas) - y[index]\n",
    "        errorThetaOne  = (univariateHypothesis(x_val, thetas) - y[index]) * x_val\n",
    "        totalErrorThetaZero += errorThetaZero\n",
    "        totalErrorThetaOne += errorThetaOne\n",
    "    \n",
    "    thetaZeroStep = thetas[0] - alpha * (1/m) * totalErrorThetaZero\n",
    "    thetaOneStep  = thetas[1] - alpha * (1/m) * totalErrorThetaOne\n",
    "    return np.array([thetaZeroStep, thetaOneStep])\n",
    "#sample data\n",
    "data = np.loadtxt(\"sampleDataLinearRegression.csv\", delimiter=',')\n",
    "x_data, y_data = data[:,0], data[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some dummy polynomial features\n",
    "For polynomial regression, we first need some polynomial data. To get that, we give you this function called `makePolynomialFeatures` that takes in a feature column and an argument `power`, that returns a DataFrame with a number of columns equal to power, where each column contains the original feature column raised to a power in `range(1, power+1)`. So `makePolynomialFeatures(np.array(featureColumn = [3,4,5], power = 3)` returns: <br> \n",
    "\\[3; 9 ; 27\\] <br>\n",
    "\\[4; 16; 64\\] <br>\n",
    "\\[5; 25; 125\\] <br>\n",
    "<br>\n",
    "\n",
    "Run the below cell to see it in action and move on to normalising the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePolynomialFeatures(x, power = 2, printit = False):\n",
    "    data = []\n",
    "    for i in range (1, power+1):\n",
    "        data.append(x**i)\n",
    "    if printit: print(data)\n",
    "    finalArray = np.vstack(tuple(data)).T\n",
    "    if printit: print(finalArray)\n",
    "    return finalArray\n",
    "\n",
    "\n",
    "        \n",
    "testPolynomial = makePolynomialFeatures(x = x_data, power = 3)\n",
    "display(testPolynomial)\n",
    "print(testPolynomial.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making a function for data normalisation\n",
    "Good. You've seen in **Exercise 1** and in the lectures that the features should be normalised to make gradient descent work well. High time to write a function to normalise these features. Call it `createNormalisedFeatures`. The function should have two arguments: `featureArray` that accepts a 2D numpy array where columns are features, and `mode` that accepts two inputs, 'SD' and 'range'. For 'SD', the normalisation should return:\n",
    "* the mean of each feature.\n",
    "* the standard deviation of each feature.\n",
    "* the normalised feature itself, using the formula: <br> $$\\frac{(feature\\ values - feature\\ mean)}{feature\\ standard\\  deviation}$$ <br>\n",
    "\n",
    "For 'range' as input the normalisation should return:\n",
    "* the mean of each feature.\n",
    "* the range of each feature (max - min).\n",
    "* the normalised feature itself, using the formula:\n",
    "$$\\frac{(feature\\ values - feature\\ mean)}{(feature\\ max - feature\\ min)}$$\n",
    "\n",
    "After you are done, test your function on a 2D Numpy array with a linear and a quadratic feature (i.e. made with `power = 2` in `makePolynomialFeatures`) using `mode = 'range'`. Code to plot the features in this space has been provided below.\n",
    "\n",
    "##### Why do we need the means and std. dev./range saved?\n",
    "\n",
    "We need these saved so if we want to predict for an unseen data point, we can transform its features in the same way we do with our training data: by subtracting the means of the training data and dividing by the std.dev./range as we do in our normalisations.\n",
    "\n",
    "##### Why these different normalisations? <br>\n",
    "The SD normalisation brings features to *approximately* the \\[-1,1\\] range, however outliers still go slightly above or below this: we normalise based on the standard deviation or average difference of the data from the mean, so points that are far from this mean will have larger values. In other words: this assumes a normal distribution of data, and shrinks that distribution to have mean 0 and *most* of its observations in the \\[-1,1\\] range. <br>\n",
    "The range normalisation brings everything to a \\[-1,1\\] range outright, no exceptions. Do note that if you have huge outliers, most data will be compressed to a tiny range, with the outliers being close to 1 or -1. Which is better? Truth be told, I don't know, and it probably depends. <br>\n",
    "<br>\n",
    "<b> Hints </b> \n",
    "* To return multiple values in one function, simply put them in a list: `return [thing1,thing2,ChickenLittle]`. In this case, order the return values like so: `[normalisedFeatures, featureMeans, featureSD/featureRanges]` <br>\n",
    "* `np.mean()`, and `np.std()` are your friends here. Be sure to set the right axis!\n",
    "* Run into broadcasting issues? Check out the `keepdims` argument.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#your answer here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plotting code\n",
    "runOnPowerTwoData = createNormalisedFeatures(makePolynomialFeatures(x_data, power = 2),\n",
    "                                          mode = \"range\")\n",
    "\n",
    "print(runOnPowerTwoData)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([0,100])\n",
    "ax.scatter(runOnPowerTwoData[0][:,0], y_data, label = \"linear feature\")\n",
    "ax.plot([np.mean(runOnPowerTwoData[0][:,0]), np.mean(runOnPowerTwoData[0][:,0])],\n",
    "       [0,100], linestyle = 'dashed', linewidth = 1, color = \"red\", label = 'mean linear feature')\n",
    "ax.scatter(runOnPowerTwoData[0][:,1], y_data, label = \"quadratic feature\")\n",
    "ax.plot([np.mean(runOnPowerTwoData[0][:,1]), np.mean(runOnPowerTwoData[0][:,1])],\n",
    "       [0,100], linestyle = 'dotted', linewidth = 1, color = \"blue\", label = 'mean quadratic feature')\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Plotting different data normalisations\n",
    "Run the below cell to see its effects on the sample linear regression data with only the normal feature (`data_x`). Note how every normalisation has slightly different characteristics (note the axis scales!).\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#set up a figure\n",
    "figNormPlot, ((ax1NormPlot, ax2NormPlot, ax3NormPlot), (ax4NormPlot, ax5NormPlot, ax6NormPlot)) = plt.subplots(2,3, figsize=(15,6.15))\n",
    "figNormPlot.tight_layout()\n",
    "\n",
    "#create normal scatterplot\n",
    "ax1NormPlot.set_title(\"Untransformed data\")\n",
    "scatter = ax1NormPlot.scatter(x_data, y_data)\n",
    "regressionLine, = ax1NormPlot.plot(x_data, 0.31 + 1.05 * x_data,\n",
    "                                   color = 'red', linestyle = \"dashed\")\n",
    "\n",
    "#range-normalised plot\n",
    "ax2NormPlot.set_title(\"Range transformed (mean-normalised) data\")\n",
    "featsRange, featsRangeMean, featsRangeRange =  createNormalisedFeatures(x_data, mode = \"range\")\n",
    "scatter = ax2NormPlot.scatter(featsRange, y_data)\n",
    "ax2NormPlot.set_xlim([-1,1])\n",
    "\n",
    "#st.dev.-normalised plot\n",
    "ax3NormPlot.set_title(\"St.Dev. transformed data\")\n",
    "featsSD, featsSDMean, featsSDSD =  createNormalisedFeatures(x_data, mode = \"SD\")\n",
    "scatter = ax3NormPlot.scatter(featsSD, y_data)\n",
    "ax3NormPlot.set_xlim([-2.5,2.5])\n",
    "\n",
    "#distribution plots. They all look the same, but note that the axis limits vary!\n",
    "sns.kdeplot(data = np.array(x_data), ax = ax4NormPlot)\n",
    "sns.kdeplot(data = np.array(featsRange), ax = ax5NormPlot)\n",
    "sns.kdeplot(data = np.array(featsSD), ax = ax6NormPlot)\n",
    "ax4NormPlot.hist(x_data, bins = 10, edgecolor='black', linewidth=1.2, density = True)\n",
    "ax5NormPlot.hist(featsRange, bins = 10, edgecolor='black', linewidth=1.2, density = True)\n",
    "ax6NormPlot.hist(featsSD, bins = 10, edgecolor='black', linewidth=1.2, density = True)\n",
    "\n",
    "#show all plots\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Towards multivariate linear regression: updating the hypothesis function\n",
    "\n",
    "Okay, now let's put it all together. First, update the hypothesis function to accomodate any number of features and parameters. It should take in the features for one sample, so a 1D array with features of length *n*, and a 1D array of thetas with length *n*, calculate $\\theta_0, \\theta_1 * x_1, ..., \\theta_n * x_n$ and return the predicted value for the data point. Call it `multiHypothesis`. <br> <br>\n",
    "\n",
    "<b> Hints </b>\n",
    "* Loop over the thetas, and multiply each with the corresponding element in x. <br>\n",
    "* In the function, you can prepend 1 to the features that the function takes in. By adding this feature that is 1, $\\theta_0$ is multiplied by 1 when you use a for loop. To do this, you can use `np.concatenate()` with `np.array([1])` as one of the arrays to be concatenated.\n",
    "* Actually you don't need the loop if you use Numpy so feel free to do it in the faster way.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array_multi = np.array([5, 10, 25])\n",
    "test_thetas_multi = np.array([0.3, 4, 2, 8]) #theta_0 means one more.\n",
    "\n",
    "#your answer here\n",
    "\n",
    "\n",
    "\n",
    "#test, answer should be 240.3\n",
    "print(np.array([1]))\n",
    "multiHypothesis(test_array_multi, test_thetas_multi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Towards multivariate regression: running gradient descent with `multiHypothesis`\n",
    "\n",
    "Armed with this glorious function, we'll take the following steps:\n",
    "* Use the `makePolynomialFeatures` function with `power = 2` to make features.\n",
    "* Normalise the features using `createNormalisedFeatures`, with `mode = 'range'`.\n",
    "* Use this normalised data in a linear regression with 2 variables by using gradient descent with this new hypothesis function. To do this, we need to change the `MyMSE` and `gradientDescent` functions to use the new `multiHypothesis` function to calculate cost. The new `MyMSE` function has been supplied as an example below. <br>\n",
    "* <b> You need to change the `gradientDescent` function to use `multiHypothesis`! </b> <br> <br>\n",
    "\n",
    "<b> Hints </b>\n",
    "* You can assume that there are only two features. <br>\n",
    "* `gradientDescent` should now return 3 values, for $\\theta_0$ , $\\theta_1$, and $\\theta_2$. Do this in a 1D Numpy array, as before.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start things off\n",
    "startThetasHP = np.array([0,0,0])\n",
    "\n",
    "featuresHP = makePolynomialFeatures(x_data, power = 2)\n",
    "\n",
    "normalisedFeaturesHP, featureMeansHP, featureRangesHP = createNormalisedFeatures(featuresHP, mode = \"range\")\n",
    "y_data = data[:,1]\n",
    "display(normalisedFeaturesHP)\n",
    "\n",
    "\n",
    "#change in MyMSE\n",
    "def MyMSE(x, y, thetas):\n",
    "    totalSumSquares = 0\n",
    "    for index, val in enumerate(x):\n",
    "        prediction = multiHypothesis(val, thetas)\n",
    "        squareError = (prediction-y[index])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(x) \n",
    "    return meanSquaredError\n",
    "\n",
    "#sample usage:\n",
    "exampleMSE = MyMSE(normalisedFeaturesHP, y_data, startThetasHP)\n",
    "print(\"MSE for thetas \" + np.array2string(startThetasHP) + \": \" + str(exampleMSE))\n",
    "\n",
    "#now it's up to you to change gradientDescent in a similar way in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running your very own multivariate linear regression algorithm\n",
    "All done? Great! You've made it through Mean-Squared Error functions, gradient descent implementations, polynomial feature generation and gradient descent that works for >1 feature. Let's see this brand spankin' new set-up in action. Of course, the underlying data is actually linear, so the quadratic equation we're fitting here will not be the best fit. But it will improve thanks to your gradient descent function-writing prowess. Run the cell below and marvel (and/or feel slightly underwhelmed) at the results!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaValuesDuringDescent = []\n",
    "MSEDuringDescent = []\n",
    "thetasNow = startThetasHP.copy()\n",
    "stepsGradDecent = 20\n",
    "alpha = 0.8\n",
    "for i in range(0,stepsGradDecent):\n",
    "    oneStep = gradientDescent(normalisedFeaturesHP, y_data, thetasNow, alpha=0.2)\n",
    "    thetaValuesDuringDescent.append(oneStep)\n",
    "    thetasNow = oneStep\n",
    "    MSEDuringDescent.append(MyMSE(normalisedFeaturesHP, y_data, thetasNow))\n",
    "#print(thetaValuesDuringDescent)\n",
    "\n",
    "fig10, ax10 = plt.subplots(figsize = (10,10))\n",
    "scatter10One = ax10.scatter(normalisedFeaturesHP[:,0], y_data)\n",
    "regressionLine10, = ax10.plot(normalisedFeaturesHP[:,0],\n",
    "                              [multiHypothesis(row, startThetasHP) for row in normalisedFeaturesHP],\n",
    "                              color = 'red', linestyle = \"dashed\",\n",
    "                              label = \"start thetas: \" + str(startThetasHP) + \" ; MSE: \" + str(np.round(exampleMSE,1)))\n",
    "\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "#repeat colours if more steps\n",
    "colors = colors * int(np.ceil(len(thetaValuesDuringDescent)/5))\n",
    "#show for all gradient steps\n",
    "for i in range(0, len(thetaValuesDuringDescent)):\n",
    "    \n",
    "    ax10.plot(normalisedFeaturesHP[:,0],\n",
    "                              [multiHypothesis(row, thetaValuesDuringDescent[i]) for row in normalisedFeaturesHP],\n",
    "                              color = colors[i], linestyle = \"dashed\", alpha = 0.8,\n",
    "                              label = \"thetas: \" + str(np.round(thetaValuesDuringDescent[i], 1)) + \" ; MSE: \" + str(np.round(MSEDuringDescent[i])))\n",
    "\n",
    "ax10.legend(bbox_to_anchor=(0, 1), loc='upper left', borderaxespad=0, fontsize = \"small\")    \n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final words\n",
    "\n",
    "\"Aaarghh\". But also: well done! Time for one more lecture, in which we'll dive into using linear algebra to make our calculations (and hopefully also our function definitions) easier, and/or further explore the bias-variance trade-off.\n",
    "\n",
    "Note: linear regression actually has an algebraically defined minimum of the cost function, which can also be approached via something called the Normal Equation (given tractable data size). Hence, linear regression libraries will probably make use of this method if feasible, and many other speed ups and other features. Still, doing it yourself is an achievement!\n",
    "\n",
    "## What I'd like you to remember here:\n",
    "* Extending linear regression to multinomial (and polynomial, i.e. with exponents) regression is relatively straightforward once you have the basic routine down.\n",
    "* How to normalise data. Especially the mean-centering and scaling to unit variance (data-mean(data))/std(data) is used very often. \n",
    "* Why normalising data is important: that otherwise you get gradients operating at different scales, messing up your gradient steps, and costs which are much higher for the polynomial variables than for the others. To avoid this, we normalise. You'll be using normalisation prodigiously throughout the rest of the course. Note that also, features that are on different scales would otherwise weigh differently in cost calculations etc. Squares of a feature that can have up to 10.000 as a value can become much higher than one that goes up only until 100.\n",
    "* That running polynomial regression on a linear relationship is a bit bonkers (but illustrative, nonetheless)\n",
    "\n",
    "\n",
    "## Survey\n",
    "Hi it's me again, the incessant reminder that you fill out the survey. Boy, surveys huh, who doesn't love 'em!? Great minds think alike, [here you go](https://docs.google.com/forms/d/e/1FAIpQLScoqJtzOclzOl8DrXnoukfySI3HAdfJNeGw_Gxplas09KdEDw/viewform?usp=sf_link)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ece01b9860ccbf107746ff1896624c399184ed0a8602a95125aa3de2311131c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
